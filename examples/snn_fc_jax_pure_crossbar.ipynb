{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Jax Implementation of a Spiking Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to provide a simple tutorial on how to implement spiking neural networks in the deep learning framework jax. For more information on jax, see https://github.com/google/jax.\n",
    "The spiking neural network will be implemented from scratch using only features available in basic jax (apart from some simple utils and a package to generate data). The spiking neural network will be trained using gradient descent together with the surrogate gradient descent method to leverage the spiking discontinuities that inevitably arise. You can find more on spiking neural networks under https://neuronaldynamics.epfl.ch/online/. The tutorial requires some prior experience with training artificial neural networks as well as a basic understanding of spiking neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "First, we import some packages that are helpful in implementing spiking neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, NamedTuple, Optional\n",
    "import functools as ft\n",
    "import tqdm\n",
    "from randman import make_spike_raster_dataset\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu') # use cpu (xbar backend is only defined for cpu host)\n",
    "import jax.numpy as jnp\n",
    "import optax # optax is a jax extension that contains implementations of popular optimizers like Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "We start by defining the shape of our neural network. For simplicity, we start only with a couple of fully connected neural networks. Will work with a popular toy dataset where points are sampled from random manifolds, called the \"Randman\". We embed these d-dimensional manifolds in an n-dimensional space and then sample points from them which serve as\n",
    "To learn more about random manifolds, look at https://direct.mit.edu/neco/article/33/4/899/97482/The-Remarkable-Robustness-of-Surrogate-Gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 2 # Number of classes\n",
    "N = [16, 32, Nc] # List of number of neurons per layer\n",
    "Nepochs = 20 # Number of training epochs\n",
    "T = 100 # Number of timesteps per epoch\n",
    "NUM_SAMPLES_PER_CLASS = 1000 \n",
    "TRAIN_TEST_SPILT = 0.8\n",
    "NUM_SAMPLES_TRAIN = int(Nc*NUM_SAMPLES_PER_CLASS*TRAIN_TEST_SPILT)\n",
    "BATCHSIZE = 48\n",
    "\n",
    "SEED = 42 \n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Creation\n",
    "\n",
    "We start by defining a dataloader for our experiment, load the data and split it into a train and a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(data, labels, batchsize, rng: Optional[np.random.Generator] = None, shuffle: bool = True):\n",
    "    \"\"\"Simple implementation of a dataloader for Jax.\"\"\"\n",
    "    num_samples = labels.shape[0]\n",
    "    num_batches = int(num_samples//batchsize)\n",
    "\n",
    "    if shuffle:\n",
    "        assert rng is not None \n",
    "        idx_samples = np.arange(num_samples)\n",
    "\n",
    "    def gen_shuffle():    \n",
    "        start, end = 0, 0\n",
    "        rng.shuffle(idx_samples)\n",
    "        for ibatch in range(num_batches):\n",
    "            end += batchsize\n",
    "            ids = idx_samples[start:end]\n",
    "            yield data[ids].transpose(1,0,2), labels[ids]\n",
    "            start = end\n",
    "\n",
    "    def gen_no_shuffle():\n",
    "        start, end = 0, 0\n",
    "        for ibatch in range(num_batches):\n",
    "            end += batchsize\n",
    "            yield data[start:end].transpose(1,0,2), labels[start:end]\n",
    "            start = end\n",
    "\n",
    "    gen = gen_shuffle if shuffle else gen_no_shuffle\n",
    "    return gen\n",
    "\n",
    "data, labels = make_spike_raster_dataset(rng, nb_classes=Nc, nb_units=N[0], \n",
    "                    nb_steps=T, step_frac=1.0, dim_manifold=2, nb_spikes=1, \n",
    "                    nb_samples=NUM_SAMPLES_PER_CLASS, alpha=2.0, shuffle=True)\n",
    "\n",
    "data_train, labels_train = data[:NUM_SAMPLES_TRAIN], labels[:NUM_SAMPLES_TRAIN]\n",
    "data_test,  labels_test  = data[NUM_SAMPLES_TRAIN:], labels[NUM_SAMPLES_TRAIN:]\n",
    "dataloader_train = create_dataloader(data_train, labels_train, BATCHSIZE, rng, shuffle=True)\n",
    "dataloader_test  = create_dataloader(data_test,  labels_test,  BATCHSIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "In this section, we define the various components that are needed to create a differential layer of stateful neurons, such that we are able to construct an arbitrary neural network consisting of fully connected spiking neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function and Surrogate Gradient Methods\n",
    "In this section, we create a smooth version of the Heaviside step function $\\Theta(x)$ such that the gradient is defined at every point. This surrogate function for the backward pass looks like:\n",
    "$\n",
    "\\Theta'(x) \\equiv \\dfrac{1}{(\\beta \\cdot |x| + 1)^2}\n",
    "$<br>\n",
    "This choice of a surrogate gradient is called \"superspike\" and is quite popular in the neuromorphic computing community. However, the model performance is robust against the actual functional form of the surrogate, see https://direct.mit.edu/neco/article/33/4/899/97482/The-Remarkable-Robustness-of-Surrogate-Gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heaviside_with_super_spike_surrogate(beta=10.):\n",
    "\n",
    "    @jax.custom_jvp\n",
    "    def heaviside_with_super_spike_surrogate(x):\n",
    "        return jnp.heaviside(x, 0)\n",
    "\n",
    "    @heaviside_with_super_spike_surrogate.defjvp\n",
    "    def f_jvp(primals, tangents):\n",
    "        x, = primals\n",
    "        x_dot, = tangents\n",
    "        primal_out = heaviside_with_super_spike_surrogate(x)\n",
    "        tangent_out = 1.0 / (beta*jnp.abs(x) + 1.0)**2 * x_dot\n",
    "        return primal_out, tangent_out\n",
    "\n",
    "    return heaviside_with_super_spike_surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we define the dynamics of the JVP, i.e. the Jacobian-Vector-Product which are equal to forward mode AD even though we are using reverse-mode AD, i.e. backpropagation for training. This is possible because Jax automatically derives the respective custom derivative rules for the reverse mode from the forward mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the Stateful Layer of Spiking Neurons within the Jax Framework\n",
    "Here, we define a simple layer of spiking neural networks, such that it can be used within the Jax framework. In particular, we have to create a custom class for the different state variables of the neurons, i.e. the membrane potential $U$, the current $I$, the recurrent current $I_r$ and the spikes $S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFDenseNeuronState(NamedTuple):\n",
    "    \"\"\"\n",
    "    Generic Module for storing the state of an RNN/SNN. \n",
    "    Each state variable is a union of a numpy array and a \n",
    "    jax numpy array to make backpropagation possible.\n",
    "    \"\"\"\n",
    "    # TODO change docstring\n",
    "    U: Union[np.ndarray, jnp.ndarray]\n",
    "    I: Union[np.ndarray, jnp.ndarray]\n",
    "    Ir: Union[np.ndarray, jnp.ndarray]\n",
    "    S: Union[np.ndarray, jnp.ndarray]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the Neuronal Dynamics\n",
    "The following class defines the dynamics of a layer of spiking neurons according to the Leaky-Integrate-and-Fire (LIF) formalism. The differential equations read:<br>\n",
    "$\n",
    "\\frac{\\mathrm{d}U^{(l)}_i}{\\mathrm{d}t} = -\\frac{1}{\\tau_\\mathrm{mem}}((U_i^{(l)} - U_\\mathrm{rest}) + R(I_i^{(l)} + I_{i,r}^{(l)})) + (U_i^{(l)} - \\vartheta)S_i^{(l)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\mathrm{d}I_i^{(l)}}{\\mathrm{d}t} = -\\frac{I_i^{(l)}}{\\tau_\\mathrm{syn}} + \\sum_j W^{(l)}_{ij} S_j^{(l-1)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\mathrm{d}I_{i, \\mathrm{r}}^{(l)}}{\\mathrm{d}t} = -\\frac{I_{i, \\mathrm{r}}^{(l)}}{\\tau_\\mathrm{syn,r}} + \\sum_j V_{ij}^{(l)}S_j^{(l)}\n",
    "$\n",
    "\n",
    "Here, $U_i^{(l)}$ is the membrane potential of the $i$th neuron in the layer $l$, $I_i^{(l)}$ is its individual feed-forward input current and $S_i^{(l-1)}$, $S_i^{(l)}$ contain the incoming spikes from the prior layer and emitted spikes form the current layer. \n",
    "Thus, the variable $I_{i, \\mathrm{r}}^{(l)}$ contains the current coming from recurrent connections within the layer and the weight matrices $W_{ij}^{(l)}$ and $V_{ij}^{(l)}$ contain the weights of the feed-forward and recurrent connections respectively.\n",
    "The parameters $\\tau_\\mathrm{mem}$, $\\tau_{syn}$ and $\\tau_{syn,r}$ are essentially decay constants that control the \"leak\" of the neurons, while $\\vartheta$ and $U_{rest}$ are the spiking threshold and resting potential of the neurons in this layer respectively. \n",
    "For more on this topic, see https://arxiv.org/pdf/1901.09948.pdf. <br>\n",
    "Pytorch by itself is not able to work with differential equations, so we need to discretize them.\n",
    "They can be discretized using the forward Euler scheme such that we arrive at equations that we can implement into our Pytorch layer: <br>\n",
    "$\n",
    "U_i^{(l)}[n+1] = \\alpha U_i^{(l)}[n] (1 - S_j^{(l)}) + (1 - \\alpha) (I_i^{(l)}[n] + I_{i, \\mathrm{r}}^{(l)}[n])\n",
    "$\n",
    "\n",
    "$\n",
    "I_i^{(l)}[n+1] = \\beta I_i^{(l)}[n] + (1 - \\beta) \\sum_j W^{(l)}_{ij} S_j^{(l-1)}[n]\n",
    "$\n",
    "\n",
    "$\n",
    "I_{i, \\mathrm{r}}^{(l)}[n+1] = \\beta_\\mathrm{r} I_{i, \\mathrm{r}}^{(l)}[n] + (1 - \\beta_\\mathrm{r}) \\sum_j V_{ij}^{(l)}S_j^{(l)}[n]\n",
    "$\n",
    "\n",
    "To implement this formula, we make use of the already present nn.Linear layer in pytorch to store the feed-forward and recurrent connection weights. Also, we set the parameters $\\vartheta = 1$, $R = 1$ and $U_\\mathrm{rest} = 0$.\n",
    "Note that this definition of the neural dynamics implies that we interpret our spiking neural network as a recurrent neural network, such that when we train it, we will have to use backpropagation through time (BPTT). This makes training spiking neural networks much more resource-demanding than training a compatible artificial neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(weights, bias, inp):\n",
    "    \"\"\"Simple implementation of a fully connected layer.\"\"\"\n",
    "    ret_val = weights @ inp\n",
    "    if bias is not None:\n",
    "        ret_val += bias \n",
    "    return ret_val\n",
    "\n",
    "def lif_step(weights, alpha, beta, betar, state, Sin_t, theta=1.0):\n",
    "    \"\"\"Simple implementation of a layer of leaky integrate-and-fire neurons.\"\"\"\n",
    "    U, I, Ir, S = state\n",
    "    fc_weight, fc_bias, rec_weight, rec_bias = weights\n",
    "\n",
    "    U = alpha*(1-jax.lax.stop_gradient(S))*U + (1-alpha)*(20.0*I+Ir) # I is weighted by a factor of 20\n",
    "    I = beta*I + (1-beta) * dense_layer(fc_weight, fc_bias, Sin_t)\n",
    "    Ir = betar*Ir + (1-betar) * dense_layer(rec_weight, rec_bias, S)\n",
    "    S_out = get_heaviside_with_super_spike_surrogate()(U-theta)\n",
    "\n",
    "    state_new = LIFDenseNeuronState(U, I, Ir, S_out)\n",
    "    return state_new, S_out\n",
    "\n",
    "def init_weights(rng: np.random.Generator, dim_in: int, dim_out: int, use_bias: bool):\n",
    "    \"\"\"A simple function to initialize the weights of a fully connected layer.\"\"\"\n",
    "    lim = (6/(dim_in+dim_out))**0.5\n",
    "    weights = rng.uniform(-lim, lim, (dim_out, dim_in))\n",
    "    bias = np.zeros(dim_out) if use_bias else None\n",
    "    return weights, bias\n",
    "\n",
    "def init_state(shape):\n",
    "    \"\"\"Function to initialize the state variables of our LIF layer.\"\"\"\n",
    "    return LIFDenseNeuronState(*[np.zeros(shape) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the definition of the dynamics for the membrane potential U, we neglect the gradient with respect to the spikes S. This is empirically known to give better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Network\n",
    "This class uses a loop to construct multiple layers of fully connected layers of spiking neurons according to the parameters given in the array $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lif_network(weights, alphas, betas, betars, initial_state, inp_spikes):\n",
    "    \"\"\"\n",
    "    Function to initialize a stack of LIF layers from given weights matrices etc.\n",
    "    It also computed the forward pass of the network for given input spikes.\n",
    "    \"\"\"\n",
    "# def lif_network(weights, initial_state, inp_spikes):\n",
    "    def step_fn_lif_network(states, spikes):\n",
    "        \"\"\"Performes a forward pass for the entire LIF network.\"\"\"\n",
    "        all_states, all_spikes = [], []\n",
    "        for params in zip(weights, alphas, betas, betars, states):\n",
    "            new_state, spikes = lif_step(*params, spikes)\n",
    "            all_states.append(new_state)\n",
    "            all_spikes.append(spikes)\n",
    "        return all_states, all_spikes\n",
    "    # TODO we should explain what jax.lax.scan does\n",
    "    final_state, out_spikes = jax.lax.scan(step_fn_lif_network, initial_state, inp_spikes)\n",
    "    return final_state, out_spikes\n",
    "\n",
    "def init_network_weights(rng, dims, use_bias_fc, use_bias_rec):\n",
    "    \"\"\"Function to initialize the weights of the entire network.\"\"\"\n",
    "    num_layers = len(dims)-1\n",
    "    all_weights = []\n",
    "    for ilay in range(num_layers):\n",
    "        fc_weights = init_weights(rng, dims[ilay], dims[ilay+1], use_bias_fc)\n",
    "        rec_weights = init_weights(rng, dims[ilay+1], dims[ilay+1], use_bias_rec)\n",
    "        all_weights.append((*fc_weights, *rec_weights))\n",
    "    return all_weights\n",
    "\n",
    "def init_network_states(batchsize, state_dims):\n",
    "    \"\"\"Function to initilize the states of every layer.\"\"\"\n",
    "    return [init_state((batchsize, dim)) for dim in state_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_network_weights(rng, N, False, False)\n",
    "NUM_LAYERS = len(N)-1\n",
    "alphas, betas, betars = [0.95]*NUM_LAYERS, [0.9]*NUM_LAYERS, [0.85]*NUM_LAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following does optional data-driven initialization to improve learning. This example uses the LSUV (layer-sequential unit variance) initialization from https://arxiv.org/abs/1511.06422. A good initialization is particularly important for spiking neural networks to make sure that we have a balance between the spiking count of all the neurons and no quiescent neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working on a classification task, it makes sense to use a cross-entropy loss to train the model. The output of the model is typically a sequence of frames containing zeros and ones to indicate whether a neuron has spiked at a certain timestep. These spikes are then summed up and we apply a softmax for them to get a probability distribution for the different classes. Thus output neuron that spiked the most gives the highest probability for the corresponding class. Encoding information in this way is called rate-coding/spike count. Furthermore we will use an ADAM optimizer to adjust the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "def one_hot_crossentropy(target_one_hot, pred):\n",
    "    \"\"\"\n",
    "    Function to calculate the softmax cross-entropy of a batch of \n",
    "    one-hot encoded target and the network output.\n",
    "    \"\"\"\n",
    "    return -jnp.sum(target_one_hot*jax.nn.log_softmax(pred)) / len(target_one_hot)\n",
    "\n",
    "def sum_and_crossentropy(one_hot_target, y_pred):\n",
    "    \"\"\"Sum the spikes over the sequence length and then calculate crossentropy.\"\"\"\n",
    "    sum_spikes = y_pred.sum(axis=0) # y_pred shape: (seq_len, batch, neurons)\n",
    "    return one_hot_crossentropy(one_hot_target, sum_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = sum_and_crossentropy\n",
    "opt = optax.sgd(2e-2, momentum=0.9) # We use stochastic gradient descent here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax is a very advanced programming library that posesses a lot of interesting features. For example, it is possible to just-in-time-compile python functions and to automatically vectorize loops for parallel execution and better performance. JIT compilation is particularly helpful for functions that are reused a lot since it directly translates them into machine code.\n",
    "Also JIT and vectorization can be arbitrarily combined. For more information about JIT, read: https://www.freecodecamp.org/news/just-in-time-compilation-explained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_single(weights, alphas, betas, betars, initial_state, inp_spikes, labels):\n",
    "    \"\"\"Function that calculates the loss for a single sample.\"\"\"\n",
    "    final_state, out_spikes = lif_network(weights, alphas, betas, betars, initial_state, inp_spikes)\n",
    "    final_layer_out_spikes = out_spikes[-1]\n",
    "    return loss_func(labels, final_layer_out_spikes)\n",
    "\n",
    "def calc_loss_batch(weights, alphas, betas, betars, initial_state, inp_spikes, labels):\n",
    "    \"\"\"\n",
    "    Function that calculates the loss for a batch of samples.\n",
    "    For this, we use vectorization through jax.vmap(...) which\n",
    "    accelerates the computations.\n",
    "    \"\"\"\n",
    "    loss_vals = jax.vmap(calc_loss_single, in_axes=(None, None, None, None, 0, 1, 0))(\n",
    "        weights, alphas, betas, betars, initial_state, inp_spikes, labels)\n",
    "    return loss_vals.sum()\n",
    "\n",
    "@jax.jit\n",
    "def calc_accuracy_batch(weights, alphas, betas, betars, initial_state, inp_spikes, labels):\n",
    "    \"\"\"\n",
    "    Function to calculate our models accuracy on the current batch.\n",
    "    This function is JIT-compiled to be faster and utilizes \n",
    "    vectorization for an even bigger speedup.\n",
    "    \"\"\"\n",
    "    _, out = jax.vmap(lif_network, in_axes=(None, None, None, None, 0, 1), out_axes=(0, 1))(weights, alphas, betas, betars, initial_state, inp_spikes)\n",
    "    sum_spikes_last = out[-1].sum(axis=0) # out shape: (seq_len, batch, neurons)\n",
    "    pred = sum_spikes_last.argmax(axis=-1) \n",
    "    return (pred==labels).mean()\n",
    "\n",
    "@jax.jit\n",
    "def update(weights, alphas, betas, betars, initial_state, inp_spikes, labels, opt_state):\n",
    "    \"\"\"\n",
    "    This function calculates the weight updates of the model by computing the gradients.\n",
    "    To speed up the process, we JIT-compile it because it will be used in every training step.\n",
    "    \"\"\"\n",
    "    loss, grads = jax.value_and_grad(calc_loss_batch)(weights, alphas, betas, betars, initial_state, inp_spikes, labels)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    # updated_weights = optax.apply_updates(weights, updates)\n",
    "    updated_weights = jax.tree_util.tree_map(lambda x, y: x+y, weights, updates)  \n",
    "    return updated_weights, opt_state, loss, updates, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath(\"\")), \"..\", \"xbarax\"))\n",
    "\n",
    "from xbarax.xla_crossbar_interface_singleBuf.custom_xla_matmul import get_mcbmm_fn\n",
    "from xbarax.xla_crossbar_interface_singleBuf.custom_xla_add import get_mcbadd_fn\n",
    "from xbarax.xla_crossbar_interface_singleBuf.custom_xla_array import MemristiveCrossbarArray, AbstractMemristiveCrossbarArray, set_memristive_crossbar_array_device_put_handler\n",
    "\n",
    "so_filename = \"../xbarax/xla_crossbar_interface_singleBuf/libfuncs.so\"\n",
    "set_memristive_crossbar_array_device_put_handler(so_filename)\n",
    "mcbmm = get_mcbmm_fn(so_filename)\n",
    "mcbadd = get_mcbadd_fn(so_filename)\n",
    "\n",
    "AbstractMemristiveCrossbarArray.set_matmul_fn(mcbmm)\n",
    "AbstractMemristiveCrossbarArray.set_add_fn(mcbadd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memristive_weight = MemristiveCrossbarArray(weights[-1][0].copy().astype(jnp.float32))\n",
    "\n",
    "# replace the last layer weight with a memristive weight\n",
    "weights = weights\n",
    "final_weights_list = list(weights[-1])\n",
    "weights[-1] = tuple((memristive_weight, *final_weights_list[1:]))\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming block, we define the training loop for our spiking neural network. As we have provided a proper gradient for the spiking discontinuities, we can train our model using gradient descent and all the other features that are available for Jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = jax.jit(opt.init)(weights)\n",
    "# opt_state = opt.init(weights)\n",
    "\n",
    "pbar = tqdm.trange(Nepochs)\n",
    "for epoch in pbar: \n",
    "    loss = 0\n",
    "    acc = []\n",
    "    # Training loop\n",
    "    for Sin, target in dataloader_train():\n",
    "        # print(\"--- train ---\")\n",
    "        initial_state = init_network_states(BATCHSIZE, N[1:])\n",
    "        targets_one_hot = create_one_hot(target, Nc, dtype=Sin.dtype)\n",
    "        weights, opt_state, loss_t, updates, grads = update(weights, alphas, betas, betars, initial_state, Sin, targets_one_hot, opt_state)\n",
    "        loss += loss_t\n",
    "    # Test loop\n",
    "    for Sin, target in dataloader_test():\n",
    "        # print(\"--- test ---\")\n",
    "        initial_state = init_network_states(BATCHSIZE, N[1:])\n",
    "        acc.append(calc_accuracy_batch(weights, alphas, betas, betars, initial_state, Sin, target))\n",
    "    pbar.set_description(f\"Training Loss {loss} | Accuracy {np.mean(acc):2.2%}: | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_0.4.6_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "56376f83bb0da5e8057e5606a730a2d02fa2fce97a89e4314223e18716284b32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
